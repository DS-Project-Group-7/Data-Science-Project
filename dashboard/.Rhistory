L <- c(1:8)
# tables of the calculated probabilities for M1 in the cases n = 25
n_samples <- 25
Compute_prob(n_samples, 5, L)
# tables of the calculated probabilities for M1 in the cases n = 100
n_samples <- 100
Compute_prob(n_samples, 5, L)
Compute_F <- function(n, p, L) {
F1 <- ((n-2*p-L)/L)*(((n-p)/(n-p-L))*exp((2*L*(1+n))/((n-p-L)*(n-p))) - 1)
F2 <- ((n-2*p-L)/L)*(((n-p)/(n-p-L))*exp((2*n*L)/((n-2*p-2)*(n-2*p-2*L-2))) - 1)
F3 <- ((n-2*p-L)/L)*(((n-p)/(n-p-L))*exp(((p+L)*log(n-p-L))/(n-p-L)-(p*log(n-p))/(n-p)) - 1)
return(cbind(F1, F2, F3))
}
Compute_prob <- function(n, p, L) {
prob <- matrix(0, 3, length(L))
for (l in L) {
F_stat <- Compute_F(n, p, l)
prob[1, l] <- pf(F_stat[1], l, n-2*p-l, lower.tail = F)
prob[2, l] <- pf(F_stat[2], l, n-2*p-l, lower.tail = F)
prob[3, l] <- pf(F_stat[3], l, n-2*p-l, lower.tail = F)
}
prob <- data.frame(prob, row.names = c("IC1", "IC2", "IC3"))
colnames(prob) <- L
return(prob)
}
L <- c(1:8)
# tables of the calculated probabilities for M1 in the cases n = 25
n_samples <- 25
Compute_prob(n_samples, 5, L)
# tables of the calculated probabilities for M1 in the cases n = 100
n_samples <- 100
Compute_prob(n_samples, 5, L)
knitr::opts_chunk$set(echo = TRUE)
library("ISLR")
data(Hitters)
# Remove rows with NA in the Salary column
hitters <- Hitters[!is.na(Hitters$Salary),]
# Construct design matrix and response variable
x <- model.matrix(Salary~., data = hitters)[,-1]
y <- hitters$Salary
lambda <- 10^seq(10, -2, length = 100)
suppressMessages(library("glmnet"))
# Estimate ridge coefficients for 100 lambda values
ridge_model <- glmnet(x, y, alpha = 0, lambda = lambda)
# Observe the coefficents for the largest lambda
coef(ridge_model)[,1]
#Observe the coefficients for the smallest lambda
coef(ridge_model)[,100]
l2norm <- rep(0, length(lambda))
for (i in 1:100) {
l2norm[i] <- norm(ridge_model$beta[,i], type="2")
}
plot(log(lambda), l2norm, xlab = "Logrithm of lambda", ylab = "l2-norm of coefficients")
# Computing the MSE for each lambda
mse <- rep(0, length(lambda))
for (i in 1:length(lambda)) {
prediction <- rep(0, length(y))
for (j in 1:length(y)) {
prediction[j] <- t(x[j,]) %*% matrix(ridge_model$beta[, i])
}
mse[i] <- mean((y - prediction)^2)
}
plot(log(lambda), mse, xlab = "Logrithm of lambda", ylab = "Mean Squared Error")
# Set the seed equal to 10 for random number generator
set.seed(10)
# Sample the training set
n_train <- sample(seq_len(length(y)), size = 131)
x_train <- x[n_train, ]
y_train <- y[n_train]
# Sample the testing set
x_test <- x[-n_train, ]
y_test <- y[-n_train]
# Performing 10-fold cross validation
(ridge_cv <- cv.glmnet(x_train, y_train, lambda = lambda, type.measure = "mse", alpha = 0))
plot(ridge_cv)
test_pred <- predict(ridge_cv$glmnet.fit, ridge_cv$lambda.min, newx = x_test)
mean((y_test - test_pred)^2)
# Refit the ridge regression model on the full data set using the lambda chosen by CV
ridge_new_model <- glmnet(x, y, alpha = 0, lambda = ridge_cv$lambda.min)
coef(ridge_new_model)
ols_model <- lm(Salary ~ ., data = Hitters)
ols_model$coefficients
set.seed(10)
(lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda, type.measure = "mse"))
plot(lasso_cv)
lasso_test_pred <- predict(lasso_cv$glmnet.fit, lasso_cv$lambda.min, newx = x_test)
mean((y_test - lasso_test_pred)^2)
lasso_new_model <- glmnet(x, y, alpha = 1, lambda = lasso_cv$lambda.min)
coef(lasso_new_model)
# Here we assume the first p samples to be zero
set.seed(10)
n_samples <- 100
M1 <- rep(0, n_samples)
M2 <- rep(0, n_samples)
# Generate samples for model M1
for (i in 6:n_samples){
M1[i] <- 0.434 * M1[i-1] + 0.217 * M1[i-2] + 0.145 * M1[i-3] + 0.108 * M1[i-4] +
0.087 * M1[i-5] + rnorm(1)
}
# Generate samples for model M2
for (i in 3:n_samples) {
M2[i] <- 0.682 * M2[i-1] + 0.346 * M2[i-2] + rnorm(1)
}
# Here we will construct the response variable and the matrix X using the samples
Create_Response <- function(n, p, model) {
y <- rep(0, n-p)
for (t in (p+1):n) {
y[t-p] <- model[t]
}
return(y)
}
Create_X <- function(n, p, model) {
X <- matrix(0, n-p, p)
for (i in 0:(p-1)) {
for (j in 1:(n-p)) {
X[j, p-i] <- model[i+j]
}
}
return(X)
}
# Compute the ICs
Compute_IC <- function(model, n, IC1, IC2, IC3, P) {
for (p in P) {
T <- n-p
tmp_y <- Create_Response(n_samples, p, model)
tmp_X <- Create_X(n_samples, p, model)
phi_hat <- solve(t(tmp_X) %*% tmp_X) %*% t(tmp_X) %*% matrix(tmp_y)
sigma2 <- sum((model[c((p+1):n)] - (tmp_X %*% phi_hat))^2)/T
# Compute the criteria
IC1[p] <- log(sigma2) + (2 * (p+1)/T)
IC2[p] <- log(sigma2) + ((T + p)/(T - p - 2))
IC3[p] <- log(sigma2) + (p * log(T)/T)
}
return(cbind(IC1, IC2, IC3))
}
P <- c(1:10)
n_samples <- 100
# Values of IC for model M1
(M1_IC <- Compute_IC(M1, n_samples, rep(0, 10), rep(0, 10), rep(0, 10), P))
# Values of IC for model M2
(M2_IC <- Compute_IC(M2, n_samples, rep(0, 10), rep(0, 10), rep(0, 10), P))
# Plotting the three criteria for model M1
plot(P, M1_IC[,1], type = "b", pch = 19, col = 'red', ylim = c(-0.5, 1.3),
main = "IC Values for M1", ylab = "IC")
lines(P, M1_IC[,2], pch = 18, col = 'blue', type = "b")
lines(P, M1_IC[,3], pch = 17, col = 'green', type = "b")
legend("bottomright", legend=c("IC1", "IC2", "IC3"),
col=c("red", "blue", "Green"), lty = 1, cex=0.8)
# Plotting the three criteria for model M2
plot(P, M2_IC[,1], type = "b", pch = 19, col = 'red', ylim = c(-0.5, 1.3),
main = "IC Values for M2", ylab = "IC")
lines(P, M2_IC[,2], pch = 18, col = 'blue', type = "b")
lines(P, M2_IC[,3], pch = 17, col = 'green', type = "b")
legend("bottomright", legend=c("IC1", "IC2", "IC3"),
col=c("red", "blue", "Green"), lty = 1, cex=0.8)
set.seed(10)
# Generate 1000 sets of size 100 using model M1
n_samples <- 100
n_sets <- 1000
M1_set_100 <- matrix(0, n_samples, n_sets)
for (i in 1:n_sets) {
M1 <- rep(0, n_samples)
for (j in 6:n_samples) {
M1[j] <- 0.434 * M1[j-1] + 0.217 * M1[j-2] + 0.145 * M1[j-3] + 0.108 * M1[j-4] +
0.087 * M1[j-5] + rnorm(1)
}
M1_set_100[, i] <- M1
}
IC_count_func <- function(model_set, n_samples, n_sets, P) {
IC_count <- matrix(0, 3, length(P))
for (i in 1:1000) {
IC <- Compute_IC(model_set[, i], n_samples,
rep(0, length(P)), rep(0, length(P)), rep(0,  length(P)), P)
IC_count[1, which(IC[,1] == min(IC[,1]))] <- IC_count[1, which(IC[,1] == min(IC[,1]))] + 1
IC_count[2, which(IC[,2] == min(IC[,2]))] <- IC_count[2, which(IC[,2] == min(IC[,2]))] + 1
IC_count[3, which(IC[,3] == min(IC[,3]))] <- IC_count[3, which(IC[,3] == min(IC[,3]))] + 1
}
IC_count <- data.frame(IC_count, row.names = c("IC1", "IC2", "IC3"))
colnames(IC_count) <- P
return(IC_count)
}
P <- c(1:10)
# Print table of counts of the selected model by IC1, IC2 and IC3
IC_count_func(M1_set_100, n_samples, n_sets, P)
set.seed(10)
# Generate 1000 sets of size 15 using model M1
n_samples <- 15
M1_set_15 <- matrix(0, n_samples, n_sets)
for (i in 1:n_sets) {
M1 <- rep(0, n_samples)
for (j in 6:n_samples) {
M1[j] <- 0.434 * M1[j-1] + 0.217 * M1[j-2] + 0.145 * M1[j-3] + 0.108 * M1[j-4] +
0.087 * M1[j-5] + rnorm(1)
}
M1_set_15[, i] <- M1
}
P <- c(1:6)
# Print table of counts of the selected model by IC1, IC2 and IC3
IC_count_func(M1_set_15, n_samples, n_sets, P)
set.seed(10)
# Generate 1000 sets of size 100 using model M2
n_samples <- 100
M2_set_100 <- matrix(0, n_samples, n_sets)
for (i in 1:n_sets) {
M2 <- rep(0, n_samples)
for (j in 3:n_samples) {
M2[j] <- 0.682 * M2[j-1] + 0.346 * M2[j-2] + rnorm(1)
}
M2_set_100[, i] <- M2
}
P <- c(1:10)
# Print table of counts of the selected model by IC1, IC2 and IC3
IC_count_func(M2_set_100, n_samples, n_sets, P)
set.seed(10)
# Generate 1000 sets of size 15 using model M2
n_samples <- 15
M2_set_15 <- matrix(0, n_samples, n_sets)
for (i in 1:n_sets) {
M2 <- rep(0, n_samples)
for (j in 3:n_samples) {
M2[j] <- 0.682 * M2[j-1] + 0.346 * M2[j-2] + rnorm(1)
}
M2_set_15[, i] <- M2
}
P <- c(1:6)
# Print table of counts of the selected model by IC1, IC2 and IC3
IC_count_func(M2_set_15, n_samples, n_sets, P)
Compute_F <- function(n, p, L) {
F1 <- ((n-p-L)/L)*(((n-p)/(n-p-L))*exp((2*L*(1+n))/((n-p-L)*(n-p))) - 1)
F2 <- ((n-p-L)/L)*(((n-p)/(n-p-L))*exp((2*n*L)/((n-2*p-2)*(n-2*p-2*L-2))) - 1)
F3 <- ((n-p-L)/L)*(((n-p)/(n-p-L))*exp(((p+L)*log(n-p-L))/(n-p-L)-(p*log(n-p))/(n-p)) - 1)
return(cbind(F1, F2, F3))
}
Compute_prob <- function(n, p, L) {
prob <- matrix(0, 3, length(L))
for (l in L) {
F_stat <- Compute_F(n, p, l)
prob[1, l] <- pf(F_stat[1], l, n-p-l, lower.tail = F)
prob[2, l] <- pf(F_stat[2], l, n-p-l, lower.tail = F)
prob[3, l] <- pf(F_stat[3], l, n-p-l, lower.tail = F)
}
prob <- data.frame(prob, row.names = c("IC1", "IC2", "IC3"))
colnames(prob) <- L
return(prob)
}
L <- c(1:8)
# tables of the calculated probabilities for M1 in the cases n = 25
n_samples <- 25
Compute_prob(n_samples, 5, L)
# tables of the calculated probabilities for M1 in the cases n = 100
n_samples <- 100
Compute_prob(n_samples, 5, L)
# Set the seed equal to 10 for random number generator
set.seed(10)
# Sample the training set
n_train <- sample(seq_len(length(y)), size = 131)
x_train <- x[n_train, ]
y_train <- y[n_train]
# Sample the testing set
x_test <- x[-n_train, ]
y_test <- y[-n_train]
# Performing 10-fold cross validation
(ridge_cv <- cv.glmnet(x_train, y_train, lambda, type.measure = "mse", alpha = 0))
# Set the seed equal to 10 for random number generator
set.seed(10)
# Sample the training set
n_train <- sample(seq_len(length(y)), size = 131)
x_train <- x[n_train, ]
y_train <- y[n_train]
# Sample the testing set
x_test <- x[-n_train, ]
y_test <- y[-n_train]
# Performing 10-fold cross validation
(ridge_cv <- cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 0))
plot(ridge_cv)
test_pred <- predict(ridge_cv$glmnet.fit, ridge_cv$lambda.min, newx = x_test)
mean((y_test - test_pred)^2)
# Refit the ridge regression model on the full data set using the lambda chosen by CV
ridge_new_model <- glmnet(x, y, alpha = 0, lambda = ridge_cv$lambda.min)
coef(ridge_new_model)
# Set the seed equal to 10 for random number generator
set.seed(10)
# Sample the training set
n_train <- sample(seq_len(length(y)), size = 131)
x_train <- x[n_train, ]
y_train <- y[n_train]
# Sample the testing set
x_test <- x[-n_train, ]
y_test <- y[-n_train]
# Performing 10-fold cross validation
(ridge_cv <- cv.glmnet(x_train, y_train, lambda = lambda, type.measure = "mse", alpha = 0))
plot(ridge_cv)
test_pred <- predict(ridge_cv$glmnet.fit, ridge_cv$lambda.min, newx = x_test)
mean((y_test - test_pred)^2)
seq_len(10)
shiny::runApp('Desktop/Data-Science-Project/dashboard')
shiny::runApp()
install.packages("shinyjs")
runApp()
library(ggplot2)
library(readxl)
library(dplyr)
library(highcharter)
art <-
read.csv(
"data/cleanData.csv"
)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade) %>%
hchart("line",
hcaes(x = decade, y = n, group = locality)) %>%
hc_xAxis(title = list(text = "Decade")) %>%
hc_yAxis(title = list(text = "Number of Paintings")) %>%
hc_legend(title = list(text = "Locality"), reversed = TRUE) %>%
hc_title(text = "Wood Type Locality Distribution Throughout the Century")%>%
hc_tooltip(pointFormat = tooltip_table(c("Locality:", "Number of paintings:"),
c("{point.locality}", "{point.y}")), useHTML = TRUE)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))
art <- read.csv("../data/cleanData.csv")[,-1]
art$locality
art <- read.csv("../data/cleanData.csv")[,-1]
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade) %>%
hchart("line",
hcaes(x = decade, y = n, group = locality)) %>%
hc_xAxis(title = list(text = "Decade")) %>%
hc_yAxis(title = list(text = "Number of Paintings")) %>%
hc_legend(title = list(text = "Locality"), reversed = TRUE) %>%
hc_title(text = "Wood Type Locality Distribution Throughout the Century")%>%
hc_tooltip(pointFormat = tooltip_table(c("Locality:", "Number of paintings:"),
c("{point.locality}", "{point.y}")), useHTML = TRUE)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"),
cs = cumsum(sales))%>%
group_by(locality) %>%
arrange(decade)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"),
cs = cumsum(locality))%>%
group_by(locality) %>%
arrange(decade)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
group_by(locality) %>%
arrange(decade)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
mutate(cs = cumsum(locality))
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
mutate(cs = cumsum(locality))%>%
group_by(locality) %>%
arrange(decade)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))
art %>%
group_by(locality) %>%
arrange(decade) %>%
mutate(cs = cumsum(locality))
is.na(art$locality)
str(art$locality)
art_test<-art %>%
mutate(locality = recode(locality, "local?" = "local", "Unspecified" = "import"))
art_test$locality
art_test%>%
group_by(locality) %>%
arrange(decade)%>%
mutate(cs = cumsum(locality))
art_test
art_test%>%
group_by(locality)
art_test%>%
group_by(locality) %>%
arrange(decade)
art_test%>%
group_by(locality,decade) %>%
arrange(decade)
art_test%>%
group_by(locality,decade) %>%
arrange(decade)%>%
mutate(cs = cumsum(locality))
art_test%>%
select(decade,locality)
art_test%>%
select(locality)
?select
art_test%>%
group_by(locality) %>%
arrange(decade)%>%
summarise(cs = sum(locality)) %>%
mutate(cs = cumsum(locality))
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))%>%
hchart("line",
hcaes(x = decade, y = n, group = locality)) %>%
hc_xAxis(title = list(text = "Decade")) %>%
hc_yAxis(title = list(text = "Number of Paintings")) %>%
hc_legend(title = list(text = "Locality"), reversed = TRUE) %>%
hc_title(text = "Cumulative sum wood type locality throughout the century")%>%
hc_tooltip(pointFormat = tooltip_table(c("Locality:", "Number of paintings:"),
c("{point.locality}", "{point.y}")), useHTML = TRUE)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))%>%
hchart("line",
hcaes(x = decade, y = cum_sum, group = locality)) %>%
hc_xAxis(title = list(text = "Decade")) %>%
hc_yAxis(title = list(text = "Number of Paintings")) %>%
hc_legend(title = list(text = "Locality"), reversed = TRUE) %>%
hc_title(text = "Cumulative sum wood type locality throughout the century")%>%
hc_tooltip(pointFormat = tooltip_table(c("Locality:", "Number of paintings:"),
c("{point.locality}", "{point.y}")), useHTML = TRUE)
runApp()
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
grouby(locality)%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
group_by(locality)%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))
test<-art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
group_by(locality)%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))
test
View(test)
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
group_by(locality)%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))%>%
hchart("line",
hcaes(x = decade, y = cum_sum, group = locality)) %>%
hc_xAxis(title = list(text = "Decade")) %>%
hc_yAxis(title = list(text = "Number of Paintings")) %>%
hc_legend(title = list(text = "Locality"), reversed = TRUE) %>%
hc_title(text = "Cumulative sum wood type locality throughout the century")%>%
hc_tooltip(pointFormat = tooltip_table(c("Locality:", "Number of paintings:"),
c("{point.locality}", "{point.y}")), useHTML = TRUE)
runApp()
runApp()
art %>%
mutate(locality =
recode(locality, "local?" = "local", "Unspecified" = "import"))%>%
group_by(locality)%>%
count(locality, decade) %>%
mutate(cum_sum = cumsum(n))%>%
hchart("areaspline",
hcaes(x = decade, y = cum_sum, group = locality)) %>%
hc_xAxis(title = list(text = "Decade")) %>%
hc_yAxis(title = list(text = "Number of Paintings")) %>%
hc_legend(title = list(text = "Locality"), reversed = TRUE) %>%
hc_title(text = "Cumulative sum wood type locality throughout the century")%>%
hc_tooltip(pointFormat = tooltip_table(c("Locality:", "Number of paintings:"),
c("{point.locality}", "{point.y}")), useHTML = TRUE)
runApp()
shiny::runApp()
shiny::runApp()
runApp()
shiny::runApp()
display_art
display_art %>% as_tibble()
display_art %>% as_tibble() %>% select_if(is.logical)
display_art %>% select(`Loose Painting Support`)
display_art %>% select("Loose Painting Support")
class(display_art)
display_art %>% as_tibble %>% select("Loose Painting Support")
display_art %>% as_tibble
display_art %>% as_tibble %>% select(`Original Suppport`)
?select
display_art %>% as_tibble %>% dplyr::select(`Original Suppport`)
runApp()
shiny::runApp()
